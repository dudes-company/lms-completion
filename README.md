# LMS Completions

**LMS Completions** is a lightweight VS Code extension that provides inline code completions using local AI models powered by **LM Studio** or **Ollama**.  
It aims to bring fast, privacy-friendly code completion to your editor with zero cloud dependencies.

---

## ğŸš€ Features

- âš¡ Inline code completions  
- ğŸ§  Works with local models (LM Studio or Ollama)  
- ğŸŒ Supports **all languages**  
- ğŸ”’ No data leaves your machine  
- ğŸ›  Simple, minimal, fast
- ğŸ“ƒ reads the whole project

---

## ğŸ“¦ Requirements

Before using the extension, make sure you have:

### Option A â€” LM Studio (recommended)
1. Download **LM Studio**  
2. Download the model:  
   **`qwen2.5-coder-0.5b-instruct`**  
3. Load the model inside LM Studio  
4. Run LM Studioâ€™s local server

### Option B â€” Ollama
1. Install **Ollama**  
2. Pull and run a supported model  
3. Ensure the Ollama server is running

---

## ğŸ§© Setup

The extension currently requires **no configuration**.  
Once LM Studio or Ollama is running

- Selecte a Code  
- Run **LM Studio: Generate Code** command
- Enjoy ğŸ¥¹

---

## ğŸ“ Known Limitations

- Completions are **not streamed yet**  


---

## ğŸ›  Roadmap

- Streaming completions  
- Model selection  
- Custom API base configuration  
- Additional providers  
- Settings UI

---

## ğŸ¤ Contributing

Contributions are **welcome and encouraged**!  
Whether you want to fix a bug, improve completion quality, add new features, or help polish the codebase â€” you're invited.

ğŸ‘‰ GitHub Repository: **https://github.com/dudes-company/lms-ai**

Feel free to submit issues, feature requests, and pull requests.

---

## ğŸ“„ License

**MIT License**
